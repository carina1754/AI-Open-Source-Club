# 1. 지도 학습과 비지도 학습
 - 분류 기준 : 학습하는 동안의 감독 형태나 정보량
 - 주요 범주 : 지도 학습, 비지도 학습, 준지도 학습, 강화 학습 
## 1.1 지도 학습
지도 학습에는 알고리즘에 주입하는 훈련 데이터에 **레이블(label)** 이라는 원하는 답이 포함됩니다<br>

전형적인 지도 학습 작업은 다음과 같습니다
- 분류 : 전형적인 지도 학습 작업.

-  타깃 수치 예측 : **예측 변수(predictor variable)** 라 부르는 **특성**을 사용하는 전형적인 지도학습 작업 (이런 종류의 작업은 **회귀(regression)** 라고 부름)

가장 중요한 지도 학습 알고리즘들
- k-최근접 이웃(k-nearest neighbors)
- 선형 회귀(linear regression)
- 로지스틱 회귀(logistic regression)
- 서포트 벡터 머신(support vector machine(SVM))
- 결정 트리(decision tree)와 랜덤 포레스트(random forest)
- 신경망(neural networks)

번외. 머신러닝에서 <u>**속성**은 데이터타입</u>을 이야기함. 일반적으로 <u>**특성**은 속성과 값</u>이 합쳐진 것을 의미함.  
또한 일부 회귀 알고리즘은 분류에 사용 가능, 반대로 일부 분류 알고리즘도 회귀에 사용 가능
<br><br>
## 1.2 비지도 학습
비지도 학습은 말그대로 훈련 데이터에 레이블이 없기에 시스템이 아무런 도움 없이 학습합니다.

가장 중요한 비지도 학습 알고리즘들
- <b>군집(clustering)</b> - 각 그룹을 더 작은 그룹으로 세분화
  -  k-평균(k-means)
  -  DBSCAN
  -  계층 군집 분석(hierarchical cluster analysis(HCA))
  -  이상치 탐지(outlier detection)와 특이치 탐지(novelty detection) <b><br>- 이상치 탐지</b> : 데이터셋에 대해 정상 또는 비정상 판단(정상 샘플로 학습 및 인식)
  <br><b>- 특이치 탐지 : </b> 데이터셋에 대해 새로운 샘플 탐지(감지하고 싶은 모든 샘플을 제거한 깨끗한 훈련 세트 필요)
  -  원-클래스(one-class SVM)
  -  아이솔레이션 포레스트(isolation forest)
- <b>시각화(visulization)와 차원 축소(dimensionality reduction)</b> 
  <b><br>- 시각화</b> : 대규모의 고차원 데이터를 넣었을 때 도식화가 가능한 2D나 3D 표현 생성. 시각화 관련 알고리즘은 한 구조를 그대로 유지하려고 하며, 데이터가 조직도 이해 및 예상하지 못한 패턴 발견 가능 
<b><br> - 차원 축소</b> : 정보 손실을 최소화하며 데이터를 간소화. 실행속도 향상과 메모리 차지 공간 줄이며 성능 향상을 위해 머신러닝 알고리즘에 사용될 데이터에 차원 축소 알고리즘을 사용하기도 함
  - 주성분 분석(principal component analysis(CPA))
  - 커널(kernel) PCA
  - 지역적 선형 임베딩(locally-linear embedding(LLE))
  - t-SNE(t-distributed stochastic neightbor embedding)
- <b>연관 규칙 학습(association rule learning)</b> - 대량의 데이터에서 특성 간의 흥미로운 관계를 찾음
  - 어프라리어리(Apriori)
  - 이클렛(Eclat)
<br><br>
## 1.3 준지도 학습
데이터에 레이블을 다는 것은 일반적으로 시간과 비용이 많이 들기 때문에 레이블이 없는 샘플도 많이 존재하고 레이블된 샘플은 적은 경우가 대부분입니다. 이런 경우 준지도 학습은 일부만 레이블이 있는 데이터를 다룰 수 있습니다.

대부분의 준지도 학습은 지도 학습과 비지도 학습의 조합으로 이루어져 있습니다. 예로 들면 심층 신뢰 신경망(deep belief network(DBN))은 여러 겹으로 쌓은 제하된 볼츠만 머신(restricted BOltzmann machine(RBM))이라 불리는 비지도 학습에 기초합니다. RBM이 비지도 학습 방식으로 순차적으로 훈련된 다음 전체 시스템이 지도 학습 방식으로 세밀하게 조정하게 됩니다.
<br><br>
## 1.4 강화 학습
강화 학습은 다른 종류의 알고리즘입니다. 여기서 부르는 학습하는 시스템을 **에이전트**라고 부르며 환경(environment)을 관찰해서 행동(action)을 실행합니다. 그 결과로 <b>보상(reward)</b> 또는 <b>벌점(penalty)</b>를 받는데 가장 큰 보상을 얻을 수 있는 <b>정책(policy)</b>라고 부르는 전략을 스스로 학습합니다.  